{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinya/demo-embedding/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  101,  7632,  1010,  2026,  3899,  2171,  2003, 23025,  4048,  1012,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "labels: tensor([[1]])\n",
      "<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.9707, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2256, -0.2689]], grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [ 2.7429e-01, -8.2388e-01,  8.1686e-01,  ..., -3.0814e-01,\n",
       "          -8.0839e-01, -6.7249e-01],\n",
       "         [ 4.6706e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n",
       "           6.9413e-01,  3.6286e-01],\n",
       "         ...,\n",
       "         [ 7.2965e-02,  2.9991e-01,  1.8814e-01,  ...,  1.5778e-01,\n",
       "           1.4888e-01,  8.9695e-01],\n",
       "         [-7.7209e-02,  2.5924e-01, -1.9776e-01,  ...,  6.0216e-01,\n",
       "           4.5769e-01,  6.1213e-01],\n",
       "         [-4.4394e-01, -2.0052e-02,  7.4488e-03,  ..., -1.1956e-01,\n",
       "           2.8498e-01,  8.9256e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 4.3804e-02, -1.4112e-02, -1.9566e-01,  ...,  2.2717e-01,\n",
       "          -1.2937e-01, -4.7297e-02],\n",
       "         [ 6.6551e-01, -7.9222e-01,  6.9617e-01,  ..., -7.3659e-01,\n",
       "          -2.7364e-01, -7.6599e-01],\n",
       "         [-3.1969e-01, -1.2195e-02, -3.2954e-01,  ..., -1.7520e-02,\n",
       "           7.2408e-01,  1.3580e-01],\n",
       "         ...,\n",
       "         [ 1.9433e-01,  3.3246e-01, -7.2570e-02,  ..., -1.6520e-01,\n",
       "           4.9985e-03,  1.1246e+00],\n",
       "         [ 4.9401e-04, -1.5600e-01, -2.2807e-01,  ...,  3.7209e-01,\n",
       "           2.1009e-01,  4.4223e-01],\n",
       "         [-1.8939e-01, -7.1956e-02, -2.9709e-02,  ...,  3.8858e-02,\n",
       "           5.2711e-01,  2.6714e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0100, -0.2898, -0.4591,  ...,  0.4876, -0.0384,  0.0565],\n",
       "         [ 1.1850, -0.2507,  1.4107,  ..., -0.4141, -0.1492, -1.0849],\n",
       "         [-0.5188,  0.0748, -0.0800,  ...,  0.0804,  0.1100, -0.0797],\n",
       "         ...,\n",
       "         [-0.3506,  0.1889, -0.0166,  ..., -0.0404, -0.3518,  1.4797],\n",
       "         [-0.1096, -0.3364,  0.2423,  ...,  0.1818,  0.1214,  0.4645],\n",
       "         [-0.1892, -0.2223,  0.1043,  ...,  0.0950,  0.3789, -0.0160]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0031, -0.3553, -0.1576,  ...,  0.4964,  0.1302,  0.1973],\n",
       "         [ 1.0442, -0.6102,  1.6731,  ..., -0.0652, -0.5458, -1.1648],\n",
       "         [-0.3453,  0.5437,  0.4202,  ...,  0.2437,  0.2685,  0.0846],\n",
       "         ...,\n",
       "         [-0.4915, -0.4996, -0.0601,  ..., -0.3955, -0.3447,  1.4967],\n",
       "         [-0.3599, -0.2619,  0.3887,  ...,  0.0816, -0.0225,  0.0215],\n",
       "         [-0.0695, -0.1083,  0.1263,  ...,  0.0704,  0.0698, -0.0300]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0620, -0.6542, -0.7677,  ...,  0.5973,  0.2352,  0.5397],\n",
       "         [ 1.0954, -0.7398,  1.5820,  ...,  0.0260, -0.4328, -1.1267],\n",
       "         [-0.6151,  0.6113,  0.3414,  ...,  0.6622,  0.2673,  0.5951],\n",
       "         ...,\n",
       "         [-0.4722, -0.6925, -0.1217,  ..., -0.2725, -0.5256,  1.4721],\n",
       "         [-0.3184, -0.3236,  0.1983,  ..., -0.1178,  0.0348,  0.3596],\n",
       "         [-0.0176, -0.0607,  0.0151,  ...,  0.0162,  0.0577, -0.0356]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0495, -0.5434, -0.6180,  ...,  0.1895,  0.2905,  0.5908],\n",
       "         [ 0.8554, -0.5067,  1.0884,  ...,  0.8250, -0.3014, -1.1648],\n",
       "         [-0.1891,  0.3482,  0.0164,  ...,  0.5779,  0.1180,  0.9997],\n",
       "         ...,\n",
       "         [-0.8023, -0.4088,  0.5671,  ..., -0.2934, -0.3895,  1.7425],\n",
       "         [-0.0648, -0.2434, -0.0741,  ...,  0.2071, -0.0744,  0.6623],\n",
       "         [-0.0159, -0.0393,  0.0127,  ...,  0.0303,  0.0164, -0.0385]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0219, -0.9178, -0.5434,  ...,  0.1368,  0.4318,  0.5411],\n",
       "         [ 0.7360, -0.5371,  1.0318,  ...,  0.8867, -0.3756, -1.0717],\n",
       "         [-0.1129,  0.3989,  0.0452,  ...,  0.2632,  0.0664,  0.8583],\n",
       "         ...,\n",
       "         [-0.8620, -0.5144,  0.4981,  ..., -0.1827, -0.0916,  1.4467],\n",
       "         [ 0.1187,  0.0871, -0.2255,  ..., -0.0448, -0.2684,  0.6588],\n",
       "         [ 0.0168, -0.0456, -0.0150,  ...,  0.0110, -0.0152, -0.0451]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.9584e-01, -4.1527e-01, -6.6038e-01,  ..., -2.1186e-01,\n",
       "           6.4528e-01,  4.5996e-01],\n",
       "         [ 3.8282e-01, -6.6090e-01,  7.7751e-01,  ...,  1.1330e+00,\n",
       "          -2.6492e-01, -1.4396e+00],\n",
       "         [-5.0097e-01,  4.1620e-01,  4.5649e-01,  ...,  5.1464e-01,\n",
       "           8.6283e-01,  6.2800e-01],\n",
       "         ...,\n",
       "         [-1.1231e+00, -6.5956e-01,  5.6802e-01,  ..., -3.7925e-01,\n",
       "          -1.8496e-01,  1.0170e+00],\n",
       "         [-1.8821e-01, -1.0921e-01, -8.6965e-01,  ..., -3.5387e-01,\n",
       "           2.1088e-01,  9.8599e-01],\n",
       "         [-7.9383e-04, -3.1244e-02, -1.8832e-02,  ..., -1.1422e-02,\n",
       "           2.4837e-02, -5.7995e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0056, -0.1457, -0.7646,  ..., -0.6087,  0.7256,  0.6783],\n",
       "         [ 0.4591, -0.2359,  0.9226,  ...,  0.6848, -0.0793, -1.0856],\n",
       "         [-0.5100,  0.0927,  0.1927,  ...,  0.4999,  0.3209,  0.6209],\n",
       "         ...,\n",
       "         [-1.4796, -0.7831,  0.1461,  ..., -0.1150,  0.1207,  0.5487],\n",
       "         [-0.6021, -0.3178, -0.9587,  ..., -0.5758, -0.1059,  0.7692],\n",
       "         [ 0.0152, -0.0118,  0.0259,  ..., -0.0076, -0.0238, -0.0795]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1995, -0.1307, -0.6693,  ..., -0.1184,  0.3797,  0.4721],\n",
       "         [ 0.3919, -0.1741,  0.8187,  ...,  0.0365,  0.2907, -0.5836],\n",
       "         [-0.5770,  0.0552,  0.3774,  ...,  0.2278,  0.2375,  0.5226],\n",
       "         ...,\n",
       "         [-1.3355, -0.7699,  0.1603,  ..., -0.1171,  0.3750,  0.3556],\n",
       "         [-0.6746, -0.1350, -0.3878,  ..., -1.1954, -0.4093,  0.1490],\n",
       "         [-0.0040, -0.0225,  0.0283,  ..., -0.0495, -0.0464, -0.0519]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-3.3721e-01, -1.3345e-01, -4.1376e-01,  ...,  3.7873e-02,\n",
       "          -1.7414e-01,  3.5817e-01],\n",
       "         [ 3.6153e-01, -6.6696e-01,  8.1522e-01,  ...,  1.9382e-01,\n",
       "           4.6599e-01, -5.0226e-01],\n",
       "         [-7.9328e-01, -2.2941e-01,  4.4029e-01,  ...,  6.2858e-02,\n",
       "           3.7651e-01,  3.6231e-01],\n",
       "         ...,\n",
       "         [-1.7260e+00, -7.3684e-01, -9.2962e-02,  ...,  1.6150e-01,\n",
       "           9.7811e-02,  3.6065e-02],\n",
       "         [-4.3740e-02, -1.5345e-02, -4.9437e-02,  ..., -1.3268e-01,\n",
       "          -3.2384e-03,  1.0226e-02],\n",
       "         [-1.8347e-02, -2.4458e-02, -2.1442e-04,  ...,  1.3963e-01,\n",
       "          -1.2944e-03, -6.4387e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0665,  0.0795, -0.0073,  ..., -0.1398,  0.1548,  0.2119],\n",
       "         [ 0.4371, -0.5470,  0.1755,  ..., -0.3620,  0.7938, -0.2025],\n",
       "         [-0.6647, -0.3503, -0.3676,  ..., -0.1766,  0.5793,  0.2944],\n",
       "         ...,\n",
       "         [-1.5236, -0.5210, -0.2938,  ...,  0.0269,  0.2586,  0.4959],\n",
       "         [ 0.0446,  0.0119, -0.0311,  ...,  0.0276, -0.0103, -0.0135],\n",
       "         [ 0.0398,  0.0089, -0.0335,  ...,  0.0320, -0.0193,  0.0033]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0182,  0.4538, -0.4096,  ..., -0.2492,  0.4402,  0.6243],\n",
       "         [ 0.3692, -0.2938, -0.2806,  ...,  0.1603,  1.3577,  0.2628],\n",
       "         [-0.2306,  0.0018, -0.4420,  ..., -0.1886,  0.5479,  0.1469],\n",
       "         ...,\n",
       "         [-1.5746, -0.4698, -0.4044,  ..., -0.8102, -0.2906,  0.1009],\n",
       "         [ 0.5818, -0.0667, -0.4743,  ...,  0.3670, -0.3840, -0.5454],\n",
       "         [ 0.7035, -0.0449, -0.4176,  ...,  0.2913, -0.5568, -0.4938]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hi, my dog name is michi.\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0) # Batch size 1\n",
    "print(f\"inputs: {inputs}\")\n",
    "print(f\"labels: {labels}\")\n",
    "print(type(model))\n",
    "outputs = model(**inputs, labels=labels, output_hidden_states=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
